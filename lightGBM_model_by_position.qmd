---
title: "lightGBM_by_position"
format: pdf
editor: visual
---

```{r}
library(ggplot2)
library(caret)
library(lightgbm)
library(stringr)
library(themis)
library(dplyr)
library(smotefamily)
```

```{r}
data <- read.csv("transfer_dataset.csv")
cleaned_data <- na.omit(data)
fw_data <- cleaned_data[cleaned_data$FW == 1,]
mf_data <- cleaned_data[cleaned_data$MF == 1,]
df_data <- cleaned_data[cleaned_data$DF == 1,]
fw_data <- fw_data %>% select(-c("player", "season"))
mf_data <- mf_data %>% select(-c("player", "season"))
df_data <- df_data %>% select(-c("player", "season"))
```

**Forward Model**

```{r}
# forward model
set.seed(278)
#fw_transfers <- fw_data[fw_data$transfer == 0,]
#fw_nontransfers <- fw_data[fw_data$transfer == 1,]
#fw_transfer_sample <- fw_transfers[sample(nrow(fw_transfers), nrow(fw_nontransfers)),]
#fw_balanced_data <- rbind(fw_transfer_sample, fw_nontransfers)
#class_penalty <- sum(fw_train_data$transfer == 0) / 
  #sum(fw_train_data$transfer == 1)
```

```{r}
set.seed(278)
fw_trainingIndex <- createDataPartition(fw_data$transfer,
                                        p = 0.7, list = FALSE)
fw_train_data <- fw_data[fw_trainingIndex,]
fw_test_data <- fw_data[-fw_trainingIndex,]
fw_train_transfers <- fw_train_data[fw_train_data$transfer == 0,]
fw_train_nontransfers <- fw_train_data[fw_train_data$transfer == 1,]
fw_train_transfer_sample <- fw_train_transfers[sample
                                                  (nrow(fw_train_transfers),
                                                      nrow
                                                    (fw_train_nontransfers)),]
fw_balanced_train <- rbind(fw_train_transfer_sample, fw_train_nontransfers)
fw_balanced_train <- fw_balanced_train[, setdiff(names(fw_balanced_train), "name")]
fw_test_data <- fw_test_data[, setdiff(names(fw_test_data), "name")]

```

```{r}
fw_lgbtrain <- lgb.Dataset(data = as.matrix(fw_balanced_train[, setdiff(names(fw_balanced_train), "transfer")]), label = fw_balanced_train$transfer, 
                           free_raw_data = FALSE)
fw_lgbtest <- lgb.Dataset(data = as.matrix(fw_test_data[, setdiff(names(fw_test_data), "transfer")]), label = fw_test_data$transfer)
```

```{r}
fw_parameters <- list(
  objective = "binary",
  metric = "auc",
  #scale_pos_weight = sum(fw_train_data$transfer == 0) / 
    #sum(fw_train_data$transfer== 1),
  boosting_type = "gbdt",
  learning_rate = 0.075,
  num_leaves = 50,
  bagging_fraction = 0.8,
  feature_fraction = 0.8,
  max_depth = 7
)
```

```{r}
fw_gbm_model <- lgb.train(
  params = fw_parameters,
  data = fw_lgbtrain,
  nrounds = 250,
  valids = list(test = fw_lgbtest),
  early_stopping_rounds = 15
)
```

```{r}
fw_predictions <- predict(fw_gbm_model,
                       as.matrix(fw_test_data[,setdiff(names(fw_test_data),
                                                    "transfer")]), label =
                         fw_test_data$transfer)
summary(fw_predictions)
fw_binary_predictions <- ifelse(fw_predictions > 0.5, 1, 0)

accuracy <- mean(fw_binary_predictions == fw_test_data$transfer)
cat("Accuracy:", accuracy, "\n")
```

```{r}
library(pROC)

fw_roc_obj <- roc(fw_test_data$transfer, fw_predictions)
fw_auc <- auc(fw_roc_obj)
```

```{r}
if (!require(knitr)) install.packages("knitr")
library(knitr)

# Compute confusion matrix
fw_conf_matrix <- confusionMatrix(factor(fw_binary_predictions),
                               factor(fw_test_data$transfer))

# Extract relevant metrics
fw_accuracy <- fw_conf_matrix$overall["Accuracy"]
fw_precision <- fw_conf_matrix$byClass["Precision"]
fw_f1_score <- fw_conf_matrix$byClass["F1"]

# Create a data frame for the metrics
fw_metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "AUC"),
  Value = unname(c(fw_accuracy, fw_precision, fw_f1_score, fw_auc))
)

# Use kable to create a formatted table
kable(fw_metrics_df, caption = "Forward Evaluation Metrics (Undersampling)")
```

```{r}

fw_roc_data <- data.frame(TPR = fw_roc_obj$sensitivities,
  FPR = 1 - fw_roc_obj$specificities,
  Thresholds = fw_roc_obj$thresholds
)
fw_roc_g <- ggplot(fw_roc_data, aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "ROC Curve - Forwards (Undersampling)",
       x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal() +
  annotate("text", x = 0.9, y = 0, label = paste("AUC =", round(fw_auc, 3)),
           size = 5, color = "black")

fw_roc_g
```

```{r}
# Calculate Precision and Recall
thresholds <- seq(0, 1, by = 0.01)
fw_precision <- sapply(thresholds, function(thresh) {
  predicted_labels <- ifelse(fw_predictions > thresh, 1, 0)
  sum(predicted_labels & fw_test_data$transfer) / sum(predicted_labels)
})
fw_recall <- sapply(thresholds, function(thresh) {
  predicted_labels <- ifelse(fw_predictions > thresh, 1, 0)
  sum(predicted_labels & fw_test_data$transfer) / sum(fw_test_data$transfer)
})

# Plot Precision-Recall Curve
fw_pr_curve_g <- plot(fw_recall, fw_precision, type = "l", col = "red", main = "Precision-Recall Curve - Forwards",
     xlim = c(0,1), ylim = c(0,1), xlab = "Recall", ylab = "Precision")
fw_pr_curve_g
```

```{r}
fw_conf_matrix <- confusionMatrix(factor(fw_binary_predictions),
                               factor(fw_test_data$transfer))

# Plot Confusion Matrix
library(ggplot2)
fw_conf_matrix <- confusionMatrix(factor(fw_binary_predictions),
                               factor(fw_test_data$transfer))

fw_conf_df <- as.data.frame(fw_conf_matrix$table)
fw_conf_df$Prediction <- factor(fw_conf_df$Prediction, levels = c(0, 1),
                             labels = c("no transfer", "transferred"))
fw_conf_df$Reference <- factor(fw_conf_df$Reference, levels = c(0,1),
                            labels = c("no transfer", "transferred"))
fw_total_obs <- sum(fw_conf_matrix$table)
fw_conf_df$Proportion <- fw_conf_df$Freq / fw_total_obs
fw_conf_matrix_g <- ggplot(data = fw_conf_df, aes(x = Prediction, y = Reference)) +
  geom_tile(fill = "white", color = "black") +
  geom_text(aes(label = paste0(Freq,
                               " (", sprintf("%.2f", Proportion * 100), "%)")), size = 5) +
  theme_minimal() +
  labs(title = "Confusion Matrix - Forwards (Undersampling)", fill = "Frequency",
       x = "Predicted Value", y = "True Value") +
  theme(axis.text.y = (element_text(size = 13, color = "black", angle = 90,
                                    hjust = 0.5)),
        axis.title.y = (element_text(size = 13, color = "black")),
        axis.text.x = (element_text(size = 13, color = "black")),
        axis.title.x = (element_text(size = 13, color = "black")))

fw_conf_matrix_g
```

```{r}
fw_importance <- lgb.importance(fw_gbm_model)
fw_importance_df <- as.data.frame(fw_importance)
fw_importance_df$Feature <- str_replace_all(fw_importance_df$Feature, "_", " ")

fw_variable_importance_g <- ggplot(fw_importance_df,
                                   aes(x = reorder(Feature, Gain),
                                                   y = Gain)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  theme_minimal() +
  scale_y_continuous(limits = c(0, 0.125)) +
  labs(title = "Feature Importance - Forwards", x = "Feature",
       y = "Gain (Improvement in Log Loss When Splitting)")

fw_variable_importance_g
```

```{r}
ggsave("undersampled_fw_unbal_pred_roc.png", plot = fw_roc_g, width = 8,
       height = 6, bg = "white")
ggsave("undersampled_fw_unbal_pred_conf_matrix.png", plot = fw_conf_matrix_g,
       width = 8, height = 6, bg = "white")
ggsave("fw_var_import.png", plot = fw_variable_importance_g, width = 8, 
       height = 6, bg = "white")
```

**Midfield Model**

```{r}
set.seed(278)
#mf_transfers <- mf_data[mf_data$transfer == 0,]
#mf_nontransfers <- mf_data[mf_data$transfer == 1,]
#mf_transfer_sample <- mf_transfers[sample(nrow(mf_transfers), nrow(mf_nontransfers)),]
#mf_balanced_data <- rbind(mf_transfer_sample, mf_nontransfers)
#mf_class_penalty <- sum(mf_train_data$transfer == 0) /
  #sum(mf_train_data$transfer == 1)
```

```{r}
set.seed(278)
mf_trainingIndex <- createDataPartition(mf_data$transfer,
                                        p = 0.7, list = FALSE)
mf_train_data <- mf_data[mf_trainingIndex,]
mf_test_data <- mf_data[-mf_trainingIndex, ]
mf_train_transfers <- mf_train_data[mf_train_data$transfer == 0,]
mf_train_nontransfers <- mf_train_data[mf_train_data$transfer == 1,]
mf_train_transfer_sample <- mf_train_transfers[sample(nrow(mf_train_transfers),
                                                      nrow
                                                      (mf_train_nontransfers)),]
mf_balanced_train <- rbind(mf_train_transfer_sample, mf_train_nontransfers)
mf_balanced_train <- mf_balanced_train[, setdiff(names(mf_balanced_train), "name")]
mf_test_data <- mf_test_data[, setdiff(names(mf_test_data), "name")]
#mf_class_penalty <- sum(mf_train_data$transfer == 0) /
  #sum(mf_train_data$transfer == 1)
```

```{r}
mf_lgbtrain <- lgb.Dataset(data = as.matrix(mf_balanced_train[, setdiff(names(mf_balanced_train), "transfer")]), label = mf_balanced_train$transfer, 
                           free_raw_data = FALSE)
mf_lgbtest <- lgb.Dataset(data = as.matrix(mf_test_data[, setdiff(names(mf_test_data), "transfer")]), label = mf_test_data$transfer)

```

```{r}
mf_parameters <- list(
  objective = "binary",
  metric = "auc",
  #scale_pos_weight = mf_class_penalty,
  boosting_type = "gbdt",
  learning_rate = 0.075,
  num_leaves = 50,
  bagging_fraction = 0.8,
  feature_fraction = 0.8,
  max_depth = 7
)
```

```{r}
mf_gbm_model <- lgb.train(
  params = mf_parameters,
  data = mf_lgbtrain,
  nrounds = 250,
  valids = list(test = mf_lgbtest),
  early_stopping_rounds = 15
)
```

```{r}
mf_predictions <- predict(mf_gbm_model,
                       as.matrix(mf_test_data[,setdiff(names(mf_test_data),
                                                    "transfer")]), label =
                         mf_test_data$transfer)
summary(mf_predictions)
mf_binary_predictions <- ifelse(mf_predictions > 0.5, 1, 0)

accuracy <- mean(mf_binary_predictions == mf_test_data$transfer)
cat("Accuracy:", accuracy, "\n")
```

```{r}
mf_roc_obj <- roc(mf_test_data$transfer, mf_predictions)
mf_auc <- auc(mf_roc_obj)
```

```{r}
mf_conf_matrix <- confusionMatrix(factor(mf_binary_predictions),
                               factor(mf_test_data$transfer))

mf_accuracy <- mf_conf_matrix$overall["Accuracy"]
mf_precision <- mf_conf_matrix$byClass["Precision"]
mf_f1_score <- mf_conf_matrix$byClass["F1"]

mf_metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "AUC"),
  Value = unname(c(mf_accuracy, mf_precision, mf_f1_score, mf_auc))
)

kable(mf_metrics_df, caption = "Midfielder Evaluation Metrics (Undersampling)")
```

```{r}
mf_roc_obj <- roc(mf_test_data$transfer, mf_predictions)
mf_auc <- auc(mf_roc_obj)

mf_roc_data <- data.frame(TPR = mf_roc_obj$sensitivities,
  FPR = 1 - mf_roc_obj$specificities,
  Thresholds = mf_roc_obj$thresholds
)
mf_roc_g <- ggplot(mf_roc_data, aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "ROC Curve - Midfielders (Undersampling)",
       x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal() + 
  annotate("text", x = 0.9, y = 0, label = paste("AUC =", round(mf_auc, 3)),
           size = 5, color = "black")

mf_roc_g
```

```{r}
thresholds <- seq(0, 1, by = 0.01)
mf_precision <- sapply(thresholds, function(thresh) {
  predicted_labels <- ifelse(mf_predictions > thresh, 1, 0)
  sum(predicted_labels & mf_test_data$transfer) / sum(predicted_labels)
})
mf_recall <- sapply(thresholds, function(thresh) {
  predicted_labels <- ifelse(mf_predictions > thresh, 1, 0)
  sum(predicted_labels & mf_test_data$transfer) / sum(mf_test_data$transfer)
})

# Plot Precision-Recall Curve
mf_pr_curve_g <- plot(mf_recall, mf_precision, type = "l", col = "red", main = "Precision-Recall Curve - Midfielders",
     xlim = c(0,1), ylim = c(0,1), xlab = "Recall", ylab = "Precision")
mf_pr_curve_g
```

```{r}
mf_conf_matrix <- confusionMatrix(factor(mf_binary_predictions),
                               factor(mf_test_data$transfer))

mf_conf_df <- as.data.frame(mf_conf_matrix$table)
mf_conf_df$Prediction <- factor(mf_conf_df$Prediction, levels = c(0, 1),
                             labels = c("no transfer", "transferred"))
mf_conf_df$Reference <- factor(mf_conf_df$Reference, levels = c(0,1),
                            labels = c("no transfer", "transferred"))
mf_total_obs <- sum(mf_conf_matrix$table)
mf_conf_df$Proportion <- mf_conf_df$Freq / mf_total_obs
mf_conf_matrix_g <- ggplot(data = mf_conf_df, aes(x = Prediction, y = Reference)) +
  geom_tile(fill = "white", color = "black") +
  geom_text(aes(label = paste0(Freq,
                               " (", sprintf("%.2f", Proportion * 100), "%)")), size = 5) +
  theme_minimal() +
  labs(title = "Confusion Matrix - Midfielders (Undersampling)",
       fill = "Frequency",
       x = "Predicted Value", y = "True Value") +
  theme(axis.text.y = (element_text(size = 13, color = "black", angle = 90,
                                    hjust = 0.5)),
        axis.title.y = (element_text(size = 13, color = "black")),
        axis.text.x = (element_text(size = 13, color = "black")),
        axis.title.x = (element_text(size = 13, color = "black")))

mf_conf_matrix_g
```

```{r}
mf_importance <- lgb.importance(mf_gbm_model)
mf_importance_df <- as.data.frame(mf_importance)
mf_importance_df$Feature <- str_replace_all(mf_importance_df$Feature, "_", " ")

mf_variable_importance_g <- ggplot(mf_importance_df,
                                   aes(x = reorder(Feature, Gain),
                                                   y = Gain)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  scale_y_continuous(limits = c(0, 0.125)) +
  theme_minimal() +
  labs(title = "Feature Importance - Midfielders", x = "Feature",
       y = "Gain (Improvement in Log Loss When Splitting)")

mf_variable_importance_g
```

```{r}
ggsave("undersampling_mf_unbal_pred_roc.png", plot = mf_roc_g, width = 8,
       height = 6, bg = "white")
ggsave("undersampling_mf_unbal_pred_conf_matrix.png", plot = mf_conf_matrix_g,
       width = 8, height = 6, bg = "white")
ggsave("mf_var_import.png", plot = mf_variable_importance_g, width = 8, 
       height = 6, bg = "white")
```

**Defenders**

```{r}
set.seed(278)
#df_transfers <- df_data[df_data$transfer == 0,]
#df_nontransfers <- df_data[df_data$transfer == 1,]
#df_transfer_sample <- df_transfers[sample(nrow(df_transfers), nrow(df_nontransfers)),]
#df_balanced_data <- rbind(df_transfer_sample, df_nontransfers)
```

```{r}
set.seed(278)
df_trainingIndex <- createDataPartition(df_data$transfer,
                                        p = 0.7, list = FALSE)
df_train_data <- df_data[df_trainingIndex,]
df_test_data <- df_data[-df_trainingIndex, ]
df_train_transfers <- df_train_data[df_train_data$transfer == 0,]
df_train_nontransfers <- df_train_data[df_train_data$transfer == 1,]
df_train_transfer_sample <- df_train_transfers[sample(nrow(df_train_transfers),
                                                      nrow
                                                      (df_train_nontransfers)),]
df_balanced_train <- rbind(df_train_transfer_sample, df_train_nontransfers)
df_balanced_train <- df_balanced_train[, setdiff(names(df_balanced_train), "name")]
df_test_data <- df_test_data[, setdiff(names(df_test_data), "name")]
#df_class_penalty <- sum(df_train_data$transfer == 0) /
  #sum(df_train_data$transfer == 1)

```

```{r}
df_lgbtrain <- lgb.Dataset(data = as.matrix(df_balanced_train[, setdiff(names(df_balanced_train), "transfer")]), label = df_balanced_train$transfer, 
                           free_raw_data = FALSE)
df_lgbtest <- lgb.Dataset(data = as.matrix(df_test_data[, setdiff(names(df_test_data), "transfer")]), label = df_test_data$transfer)
```

```{r}
df_parameters <- list(
  objective = "binary",
  metric = "auc",
  #scale_pos_weight = df_class_penalty,
  boosting_type = "gbdt",
  learning_rate = 0.075,
  num_leaves = 50,
  bagging_fraction = 0.8,
  feature_fraction = 0.8,
  max_depth = 7
)
```

```{r}
df_gbm_model <- lgb.train(
  params = df_parameters,
  data = df_lgbtrain,
  nrounds = 250,
  valids = list(test = df_lgbtest),
  early_stopping_rounds = 15
)
```

```{r}
df_predictions <- predict(df_gbm_model,
                       as.matrix(df_test_data[,setdiff(names(df_test_data),
                                                    "transfer")]), label =
                         df_test_data$transfer)
summary(df_predictions)
df_binary_predictions <- ifelse(df_predictions > 0.5, 1, 0)

accuracy <- mean(df_binary_predictions == df_test_data$transfer)
cat("Accuracy:", accuracy, "\n")
```

```{r}
df_roc_obj <- roc(df_test_data$transfer, df_predictions)
df_auc <- auc(df_roc_obj)
```

```{r}
df_conf_matrix <- confusionMatrix(factor(df_binary_predictions),
                               factor(df_test_data$transfer))

df_accuracy <- df_conf_matrix$overall["Accuracy"]
df_precision <- df_conf_matrix$byClass["Precision"]
df_f1_score <- df_conf_matrix$byClass["F1"]


df_metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "AUC"),
  Value = unname(c(df_accuracy, df_precision, df_f1_score, df_auc))
)

kable(df_metrics_df, caption = "Defender Evaluation Metrics (Undersampling)")
```

```{r}
df_roc_data <- data.frame(TPR = df_roc_obj$sensitivities,
  FPR = 1 - df_roc_obj$specificities,
  Thresholds = df_roc_obj$thresholds
)
df_roc_g <- ggplot(df_roc_data, aes(x = FPR, y = TPR)) +
  geom_line(color = "blue") + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "ROC Curve - Defenders (Undersampling)",
       x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal() +
  annotate("text", x = 0.9, y = 0, label = paste("AUC =", round(df_auc, 3)),
           size = 5, color = "black")

df_roc_g
```

```{r}
thresholds <- seq(0, 1, by = 0.01)
df_precision <- sapply(thresholds, function(thresh) {
  predicted_labels <- ifelse(df_predictions > thresh, 1, 0)
  sum(predicted_labels & df_test_data$transfer) / sum(predicted_labels)
})
df_recall <- sapply(thresholds, function(thresh) {
  predicted_labels <- ifelse(df_predictions > thresh, 1, 0)
  sum(predicted_labels & df_test_data$transfer) / sum(df_test_data$transfer)
})

# Plot Precision-Recall Curve
df_pr_curve_g <- plot(df_recall, df_precision, type = "l", col = "red", main = "Precision-Recall Curve - Defenders",
     xlim = c(0,1), ylim = c(0,1), xlab = "Recall", ylab = "Precision")
df_pr_curve_g
```

```{r}
df_conf_matrix <- confusionMatrix(factor(df_binary_predictions),
                               factor(df_test_data$transfer))

df_conf_df <- as.data.frame(df_conf_matrix$table)
df_conf_df$Prediction <- factor(df_conf_df$Prediction, levels = c(0, 1),
                             labels = c("no transfer", "transferred"))
df_conf_df$Reference <- factor(df_conf_df$Reference, levels = c(0,1),
                            labels = c("no transfer", "transferred"))
df_total_obs <- sum(df_conf_matrix$table)
df_conf_df$Proportion <- df_conf_df$Freq / df_total_obs
df_conf_matrix_g <- ggplot(data = df_conf_df, aes(x = Prediction, y = Reference)) +
  geom_tile(fill = "white", color = "black") +
  geom_text(aes(label = paste0(Freq,
                               " (", sprintf("%.2f", Proportion * 100), "%)")), size = 5) +
  theme_minimal() +
  labs(title = "Confusion Matrix - Defenders (Undersampling)",
       fill = "Frequency",
       x = "Predicted Value", y = "True Value") +
  theme(axis.text.y = (element_text(size = 13, color = "black", angle = 90,
                                    hjust = 0.5)),
        axis.title.y = (element_text(size = 13, color = "black")),
        axis.text.x = (element_text(size = 13, color = "black")),
        axis.title.x = (element_text(size = 13, color = "black")))

df_conf_matrix_g
```

```{r}
df_importance <- lgb.importance(df_gbm_model)
df_importance_df <- as.data.frame(df_importance)
df_importance_df$Feature <- str_replace_all(df_importance_df$Feature, "_", " ")

df_variable_importance_g <- ggplot(df_importance_df,
                                   aes(x = reorder(Feature, Gain),
                                                   y = Gain)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  #scale_y_continuous(limits = c(0, 0.125)) +
  theme_minimal() +
  labs(title = "Feature Importance - Defenders", x = "Feature",
       y = "Gain (Improvement in Log Loss When Splitting)")

df_variable_importance_g
```

```{r}
ggsave("undersampling_df_unbal_pred_roc.png", plot = df_roc_g, width = 8,
       height = 6, bg = "white")
ggsave("undersampling_df_unbal_pred_conf_matrix.png", plot = df_conf_matrix_g,
       width = 8, height = 6, bg = "white")
ggsave("df_var_import.png", plot = df_variable_importance_g, width = 8, 
       height = 6, bg = "white")
```

Notes for remaining analysis:

Ask about whether prediction set should be balanced or not (I think so)

Try oversampling, compare results to undersampling and if there are changes try to explain them (and if all are bad, look for explanations for that)
