---
title: "Random Forest, Visuals"
author: "Andrew Henderson"
date: "2025-03-31"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
if (!require(randomForest)) install.packages("randomForest", dependencies = TRUE)
if (!require(caret)) install.packages("caret", dependencies = TRUE)

library(randomForest)
library(caret)
library(dplyr)
library(kableExtra)
library(grid)
library(gridExtra)
library(stringr)
```

Random Forest Models / Position

```{r}
positions <- c("MF", "FW", "DF")

models <- list()
conf_matrices <- list()

for (pos in positions) {
  
  pos_data <- subset(data, data[[pos]] == 1)
  pos_data <- pos_data[, !(names(pos_data) %in% c("season", "player", "MD", "FW", "DF"))]
  pos_data$transfer <- as.factor(pos_data$transfer)
  
  if (nrow(pos_data) < 10) {
    cat("Not enough data for position", pos, "\n")
    next
  }
  
  # upSample to upsample ds
  # downSample to downsample ds
  ds <- upSample(x = pos_data[, setdiff(names(pos_data), "transfer")],
                   y = pos_data$transfer,
                   yname = "transfer")
  
  # Split the downsampled data into training and testing sets
  set.seed(42)
  trainIndex <- createDataPartition(ds$transfer, p = 0.7, list = FALSE)
  trainData <- ds[trainIndex, ]
  testData <- ds[-trainIndex, ]
  
  # Train the Random Forest model on the balanced dataset
  rf_model <- randomForest(transfer ~ ., 
                           data = trainData, 
                           ntree = 500, 
                           mtry = floor(sqrt(ncol(trainData) - 1)), 
                           importance = TRUE)
  
  models[[pos]] <- rf_model
  
  # Make predictions on the test set
  predictions <- predict(rf_model, testData)
  
  # Evaluate the model performance
  conf_mat <- confusionMatrix(predictions, testData$transfer)
  conf_matrices[[pos]] <- conf_mat
  
  cat("Confusion Matrix for", pos, "players:\n")
  print(conf_mat)
  cat("\n---------------------------------\n")
}
```

```{r}
#MF players:
cat("Model summary for MF players:\n")
print(models[["MF"]])
plot(models[["MF"]], main = "OOB Error for MD Players")
varImpPlot(models[["MF"]], main = "Variable Importance for MD Players")

#FW players:
cat("Model summary for FW players:\n")
print(models[["FW"]])
plot(models[["FW"]], main = "OOB Error for FW Players")
varImpPlot(models[["FW"]], main = "Variable Importance for FW Players")


#DF players:
cat("Model summary for DF players:\n")
print(models[["DF"]])
plot(models[["DF"]], main = "OOB Error for DF Players")
varImpPlot(models[["DF"]], main = "Variable Importance for DF Players")

```

```{r}
oob_data <- data.frame(
  trees = 1:nrow(models[["MF"]]$err.rate),
  OOB_Error = models[["MF"]]$err.rate[,"OOB"]
)

library(ggplot2)
ggplot(oob_data, aes(x = trees, y = OOB_Error)) +
  geom_line(color = "blue", size = 1.2) +
  labs(
    title = "OOB Error Rate for MF Players",
    x = "Number of Trees",
    y = "OOB Error"
  ) +
  theme_minimal()

importanceMF <- data.frame(
  Variable = rownames(models[["MF"]]$importance),
  Importance = models[["MF"]]$importance[,"MeanDecreaseAccuracy"]
)

# Order variables by importance
importanceMF <- importanceMF[order(importanceMF$Importance, decreasing = TRUE), ]
importanceMF$Variable <- str_to_title(gsub("_", " ", importanceMF$Variable))
importanceMF$Color <- rep(c("steelblue", "tomato"), length.out = nrow(importanceMF))

ggplot(importanceMF, aes(x = reorder(Variable, Importance), y = Importance, fill = Color)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "Variable Importance for Midfielders (MDA)",
    x = "Variables",
    y = "Importance"
  ) +
  scale_fill_identity() +
  theme_minimal()

importanceDF <- data.frame(
  Variable = rownames(models[["DF"]]$importance),
  Importance = models[["DF"]]$importance[,"MeanDecreaseAccuracy"]
)

# Order variables by importance
importanceDF<- importanceDF[order(importanceDF$Importance, decreasing = TRUE), ]
importanceDF$Variable <- str_to_title(gsub("_", " ", importanceDF$Variable))
importanceDF$Color <- rep(c("steelblue", "tomato"), length.out = nrow(importanceDF))

ggplot(importanceDF, aes(x = reorder(Variable, Importance), y = Importance, fill = Color)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "Variable Importance for Defenders (MDA)",
    x = "Variables",
    y = "Importance"
  ) +
  scale_fill_identity() +
  theme_minimal()

importanceFW <- data.frame(
  Variable = rownames(models[["FW"]]$importance),
  Importance = models[["FW"]]$importance[,"MeanDecreaseAccuracy"]
)

# Order variables by importance
importanceFW <- importanceFW[order(importanceFW$Importance, decreasing = TRUE), ]
importanceFW$Variable <- str_to_title(gsub("_", " ", importanceFW$Variable))
importanceFW$Color <- rep(c("steelblue", "tomato"), length.out = nrow(importanceFW))

ggplot(importanceFW, aes(x = reorder(Variable, Importance), y = Importance, fill = Color)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "Variable Importance for Forwards (MDA)",
    x = "Variables",
    y = "Importance"
  ) +
  scale_fill_identity() +
  theme_minimal()
```
```{r}
performance_table <- do.call(rbind, lapply(names(conf_matrices), function(pos) {
  cm <- conf_matrices[[pos]]
  data.frame(
    Position = pos,
    Accuracy = round(as.numeric(cm$overall["Accuracy"]), 3),
    Sensitivity = round(as.numeric(cm$byClass["Sensitivity"]), 3),
    Specificity = round(as.numeric(cm$byClass["Specificity"]), 3)
  )
}))

kable(performance_table, 
      caption = "RF Model Performance Metrics by Position",
      col.names = c("Position", "Accuracy", "Sensitivity", "Specificity"))

performance_table <- do.call(rbind, lapply(names(conf_matrices), function(pos) {
  cm <- conf_matrices[[pos]]
  data.frame(
    Position = pos,
    Accuracy = round(as.numeric(cm$overall["Accuracy"]), 3),
    Sensitivity = round(as.numeric(cm$byClass["Sensitivity"]), 3),
    Specificity = round(as.numeric(cm$byClass["Specificity"]), 3)
  )
}))

performance_table <- do.call(rbind, lapply(names(conf_matrices), function(pos) {
  cm <- conf_matrices[[pos]]
  data.frame(
    Position = pos,
    Accuracy = round(as.numeric(cm$overall["Accuracy"]), 3),
    Sensitivity = round(as.numeric(cm$byClass["Sensitivity"]), 3),
    Specificity = round(as.numeric(cm$byClass["Specificity"]), 3)
  )
}))

# Create a title for the summary table
summary_title <- textGrob("RF Model Performance Metrics by Position",
                            gp = gpar(fontsize = 14, fontface = "bold"))

# Convert the summary data frame into a table graphic
performance_table_grob <- tableGrob(performance_table)

# Arrange the title and table vertically
grid.arrange(summary_title, performance_table_grob, 
             nrow = 2, 
             heights = c(0.15, 0.85))


# Now, loop over each confusion matrix to display it nicely
for (pos in names(conf_matrices)) {
  # Create a title for each confusion matrix
  cm_title <- textGrob(paste("Confusion Matrix for", pos, "Players"),
                       gp = gpar(fontsize = 14, fontface = "bold"))
  # Convert the confusion matrix table into a table graphic
  cm_grob <- tableGrob(as.table(conf_matrices[[pos]]$table))
  
  # Arrange the title and the matrix table together
  grid.arrange(cm_title, cm_grob, 
               nrow = 2, 
               heights = c(0.15, 0.85))
}

```
```{r}
importance_data <- data.frame(
  Variable = rownames(models[["MF"]]$importance),
  Importance = models[["MF"]]$importance[,"MeanDecreaseAccuracy"]
)

# Remove underscores, replace with spaces, and convert text to title case
importance_data$Variable <- str_to_title(gsub("_", " ", importance_data$Variable))

# Order the data by importance, descending
importance_data <- importance_data[order(importance_data$Importance, decreasing = TRUE), ]

# Create an alternating color vector (for example, between "steelblue" and "tomato")
importance_data$Color <- rep(c("steelblue", "tomato"), length.out = nrow(importance_data))

# Plot the variable importance using ggplot2 with the cleaned labels
ggplot(importance_data, aes(x = reorder(Variable, Importance), y = Importance, fill = Color)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "Variable Importance (MeanDecreaseAccuracy)",
    x = "Variables",
    y = "Importance"
  ) +
  scale_fill_identity() +
  theme_minimal()

importance_data <- data.frame(
  Variable = rownames(models[["MF"]]$importance),
  Importance = models[["MF"]]$importance[,"MeanDecreaseAccuracy"]
)

# Remove underscores, replace with spaces, and convert text to title case
importance_data$Variable <- str_to_title(gsub("_", " ", importance_data$Variable))

# Order the data by importance, descending
importance_data <- importance_data[order(importance_data$Importance, decreasing = TRUE), ]

# Create an alternating color vector (for example, between "steelblue" and "tomato")
importance_data$Color <- rep(c("steelblue", "tomato"), length.out = nrow(importance_data))

# Plot the variable importance using ggplot2 with the cleaned labels
ggplot(importance_data, aes(x = reorder(Variable, Importance), y = Importance, fill = Color)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "Variable Importance (MeanDecreaseAccuracy)",
    x = "Variables",
    y = "Importance"
  ) +
  scale_fill_identity() +
  theme_minimal()
```

### Validation Testing

```{r}
set.seed(42)  # for reproducibility

positions <- c("MF", "FW", "DF")

models       <- list()
conf_matrices<- list()
validations  <- list()

for (pos in positions) {
  
  # 1. Subset and clean
  pos_data <- subset(data, data[[pos]] == 1)
  pos_data <- pos_data[, !(names(pos_data) %in% c("season","player","MD","FW","DF"))]
  pos_data$transfer <- factor(pos_data$transfer)
  
  if (nrow(pos_data) < 10) {
    cat("Not enough data for position", pos, "\n")
    next
  }
  
  # 2. Create a stratified validation set (e.g. 10% of pos_data)
  val_idx        <- createDataPartition(pos_data$transfer, p = 0.10, list = FALSE)
  validation    <- pos_data[val_idx, ]
  remaining     <- pos_data[-val_idx, ]
  validations[[pos]] <- validation
  
  # 3. Balance the *remaining* data
  ds <- upSample(
    x     = remaining[, setdiff(names(remaining), "transfer")],
    y     = remaining$transfer,
    yname = "transfer"
  )
  
  # 4. Split the balanced dataset into training (70%) and testing (30%)
  train_idx <- createDataPartition(ds$transfer, p = 0.70, list = FALSE)
  trainData <- ds[train_idx, ]
  testData  <- ds[-train_idx, ]
  
  # 5. Train RF on the training set
  rf_model <- randomForest(
    transfer ~ .,
    data = trainData,
    ntree = 500,
    mtry  = floor(sqrt(ncol(trainData) - 1)),
    importance = TRUE
  )
  models[[pos]] <- rf_model
  
  # 6. Evaluate on test set
  preds <- predict(rf_model, testData)
  conf  <- confusionMatrix(preds, testData$transfer)
  conf_matrices[[pos]] <- conf
  
  # 7. (Optional) Evaluate on your untouched validation set
  val_preds <- predict(rf_model, validation)
  val_conf  <- confusionMatrix(val_preds, validation$transfer)
  
  cat("=== Position:", pos, "===\n")
  print(conf)
  cat("Validation performance:\n")
  print(val_conf)
  cat("\n------------------------------\n")
}

```

```{r}
positions       <- c("MF", "FW", "DF")
models_up       <- list()
models_down     <- list()
conf_up         <- list()
conf_down       <- list()
validations     <- list()

for (pos in positions) {
  
  # 1. Subset and clean
  pos_data <- subset(data, data[[pos]] == 1)
  pos_data <- pos_data[, !(names(pos_data) %in% c("season","player","MD","FW","DF"))]
  pos_data$transfer <- factor(pos_data$transfer)
  
  if (nrow(pos_data) < 10) {
    cat("Not enough data for position", pos, "\n")
    next
  }
  
  # 2. Hold out a 10% validation set (stratified)
  val_idx       <- createDataPartition(pos_data$transfer, p = 0.20, list = FALSE)
  validation    <- pos_data[val_idx, ]
  remaining     <- pos_data[-val_idx, ]
  validations[[pos]] <- validation
  
  # 3A. Up-sample the minority class in the remaining data
  ds_up <- upSample(
    x     = remaining[, setdiff(names(remaining), "transfer")],
    y     = remaining$transfer,
    yname = "transfer"
  )
  
  # 3B. Down-sample the majority class in the remaining data
  ds_down <- downSample(
    x     = remaining[, setdiff(names(remaining), "transfer")],
    y     = remaining$transfer,
    yname = "transfer"
  )
  
  # 4. Function to train & evaluate
  run_rf <- function(ds) {
    idx   <- createDataPartition(ds$transfer, p = 0.70, list = FALSE)
    train <- ds[idx, ]
    test  <- ds[-idx, ]
    
    rf <- randomForest(
      transfer ~ .,
      data       = train,
      ntree      = 500,
      mtry       = floor(sqrt(ncol(train) - 1)),
      importance = TRUE
    )
    preds     <- predict(rf, test)
    cm        <- confusionMatrix(preds, test$transfer)
    list(model = rf, cm = cm)
  }
  
  # 5A. Train & eval on up-sampled
  res_up          <- run_rf(ds_up)
  models_up[[pos]]   <- res_up$model
  conf_up[[pos]]   <- res_up$cm
  
  # 5B. Train & eval on down-sampled
  res_down        <- run_rf(ds_down)
  models_down[[pos]] <- res_down$model
  conf_down[[pos]] <- res_down$cm
  
  # 6. Print results
  cat("\n---", pos, " (UPSAMPLED) ---\n")
  print(conf_up[[pos]])
  cat("\n---", pos, " (DOWNSAMPLED) ---\n")
  print(conf_down[[pos]])
  cat("\n===========================\n")
}
```

```{r}
for (pos in positions) {
  cat("\n=== Position:", pos, "===\n")
  
  val_data <- validations[[pos]]
  if (is.null(val_data) || nrow(val_data) == 0) {
    cat("  No validation data for", pos, "\n"); next
  }
  
  # Upsampled model
  if (!is.null(models_up[[pos]])) {
    preds_up    <- predict(models_up[[pos]], val_data)
    cm_val_up   <- confusionMatrix(preds_up, val_data$transfer)
    cat("\n-- Validation (upsampled training) --\n")
    print(cm_val_up)
  }
  
  # Downsampled model
  if (!is.null(models_down[[pos]])) {
    preds_down  <- predict(models_down[[pos]], val_data)
    cm_val_down <- confusionMatrix(preds_down, val_data$transfer)
    cat("\n-- Validation (downsampled training) --\n")
    print(cm_val_down)
  }
  
  cat("\n------------------------------\n")
}
```
```{r}
perf_up <- do.call(rbind, lapply(names(conf_up), function(pos) {
  cm <- conf_up[[pos]]
  data.frame(
    Position    = pos,
    Sampling    = "Up-sampled",
    Accuracy    = round(as.numeric(cm$overall["Accuracy"]),    3),
    Sensitivity = round(as.numeric(cm$byClass["Sensitivity"]), 3),
    Specificity = round(as.numeric(cm$byClass["Specificity"]), 3),
    stringsAsFactors = FALSE
  )
}))

perf_down <- do.call(rbind, lapply(names(conf_down), function(pos) {
  cm <- conf_down[[pos]]
  data.frame(
    Position    = pos,
    Sampling    = "Down-sampled",
    Accuracy    = round(as.numeric(cm$overall["Accuracy"]),    3),
    Sensitivity = round(as.numeric(cm$byClass["Sensitivity"]), 3),
    Specificity = round(as.numeric(cm$byClass["Specificity"]), 3),
    stringsAsFactors = FALSE
  )
}))

performance_val <- rbind(perf_up, perf_down)

# 2. Render it as a kable
kable(performance_val,
      caption   = "Validation Set Performance: Up- vs. Down-sampled RF Models",
      col.names = c("Position", "Sampling", "Accuracy", "Sensitivity", "Specificity")) %>%
  kable_styling(full_width = FALSE, position = "center")

# 3. Turn it into a grob for plotting
title_perf <- textGrob("Validation Set Performance: Up- vs. Down-sampled RF Models",
                       gp = gpar(fontsize = 14, fontface = "bold"))
perf_grob  <- tableGrob(performance_val)

grid.newpage()
grid.arrange(title_perf, perf_grob, 
             nrow = 2, heights = c(0.15, 0.85))

# 4. Plot each confusion matrix from validation for both sampling strategies
for (pos in intersect(names(conf_up), names(conf_down))) {
  # Up-sampled
  cm_up_grob <- tableGrob(as.table(conf_up[[pos]]$table))
  title_up   <- textGrob(paste(pos, "- Up-sampled Validation"), 
                         gp = gpar(fontsize = 13, fontface = "bold"))
  grid.newpage()
  grid.arrange(title_up, cm_up_grob, nrow = 2, heights = c(0.15, 0.85))
  
  # Down-sampled
  cm_down_grob <- tableGrob(as.table(conf_down[[pos]]$table))
  title_down   <- textGrob(paste(pos, "- Down-sampled Validation"), 
                           gp = gpar(fontsize = 13, fontface = "bold"))
  grid.newpage()
  grid.arrange(title_down, cm_down_grob, nrow = 2, heights = c(0.15, 0.85))
}
```

